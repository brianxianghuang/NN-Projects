import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split

class Perceptron:

    def __init__(self, learning_rate=0.01, n_iters=100):
        self.lr = learning_rate
        self.n_iters = n_iters
        self.activation_func = self.unit_step_func
        self.weights = None
        self.bias = 0.5

    def unit_step_func(self, x):
        act = self.bias
        act += x
        return np.where(act > 0, 1, -1)

    def fit(self, X, y):
        n_samples, n_features = X.shape

        # 初始化參數
        self.weights = np.array([-0.5,0.5])
        y_ = np.where(y > 0, 1, -1)

        # 學習權重
        for _ in range(self.n_iters):
            for idx, x_i in enumerate(X):
                linear_output = np.dot(x_i, self.weights)
                y_predicted = self.activation_func(linear_output)

                # 感知器更新規則
                update = self.lr * (y_[idx] - y_predicted)
                self.weights += update * x_i
                self.bias += update

    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        y_predicted = self.activation_func(linear_output)
        return y_predicted


# Load the Wine dataset
wine = datasets.load_wine()
# We'll use only two classes (0 and 1) and the features 'Alcohol' and 'Malic acid'
X = wine.data[wine.target != 2, :2]
y = wine.target[wine.target != 2]
# Convert class 0 to -1 for perceptron implementation
y[y == 0] = -1
# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Parameters
learning_rate = 0.01
epochs = 500
weights = np.array([-0.5, 0.5])
bias = 0.5

# Create a Perceptron object and train it
p = Perceptron(learning_rate=learning_rate, n_iters=epochs)
p.fit(X_train, y_train)

# Predict on test data
predictions = p.predict(X_test)

# Calculate accuracy
accuracy = np.sum(predictions == y_test)/len(y_test)
print(f"Accuracy: {accuracy * 100:.2f}%")

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)
plt.scatter(X_train[:, 0], X_train[:, 1], marker="o", c=y_train)

x0_1 = np.amin(X_train[:, 0])
x0_2 = np.amax(X_train[:, 0])

x1_1 = (-p.weights[0] * x0_1 - p.bias) / p.weights[1]
x1_2 = (-p.weights[0] * x0_2 - p.bias) / p.weights[1]

ax.plot([x0_1, x0_2], [x1_1, x1_2], "k")

ymin = np.amin(X_train[:, 1])
ymax = np.amax(X_train[:, 1])
ax.set_ylim([ymin - 3, ymax + 3])

plt.show()
